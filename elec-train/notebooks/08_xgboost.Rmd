---
title: "08 - XGBoost"
author: "Time Series Analysis"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

This notebook implements XGBoost for electricity consumption forecasting with:
- Feature selection based on significant PACF lags
- Temperature as exogenous variable
- Hyperparameter tuning

# Load Required Libraries

```{r libraries}
if (!require("xgboost")) install.packages("xgboost")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("Metrics")) install.packages("Metrics")
if (!require("forecast")) install.packages("forecast")
if (!require("tidyr")) install.packages("tidyr")

library(xgboost)
library(ggplot2)
library(Metrics)
library(forecast)
library(tidyr)
```

# Load Data

```{r load-data}
load("../data/elec_data.RData")

cat("Training set length:", length(train_ts), "\n")
cat("Validation set length:", length(validation_ts), "\n")
cat("Training temperature length:", length(train_temp_ts), "\n")
cat("Validation temperature length:", length(validation_temp_ts), "\n")
cat("Forecast horizon:", horizon, "\n")
```

# Feature Selection Based on PACF

```{r pacf-analysis, fig.width=12, fig.height=6}
# Analyze PACF to identify significant lags
ggtsdisplay(train_ts, lag.max = 200, main = "Training Data - ACF/PACF")
```

Based on PACF analysis, significant lags are typically around:
- **1, 37, 61**: Short-term dependencies  
- **96, 97**: Daily seasonality (lag 96 = 24h)
- **192**: Two-day pattern

```{r define-lags}
# Significant lags from PACF (similar to your friend's analysis)
lags <- c(1, 37, 61, 96, 97, 192)

cat("Selected lags:", paste(lags, collapse = ", "), "\n")
cat("Max lag:", max(lags), "\n")
```

# Data Preparation

```{r data-prep}
# Function to create features from lagged values + temperature
create_features <- function(power_data, temp_data, lags) {
  n <- length(power_data)
  max_lag <- max(lags)
  m <- n - max_lag
  
  # Create lagged features matrix
  X <- matrix(NA, nrow = m, ncol = length(lags))
  colnames(X) <- paste0("lag_", lags)
  
  for (i in seq_along(lags)) {
    lag <- lags[i]
    X[, i] <- power_data[(max_lag - lag + 1):(n - lag)]
  }
  
  # Add temperature feature
  temperature <- temp_data[(max_lag + 1):n]
  X <- cbind(X, temperature = temperature)
  
  # Target
  y <- power_data[(max_lag + 1):n]
  
  return(list(X = X, y = y, max_lag = max_lag))
}

# Create training data
train_power <- as.numeric(train_ts)
train_temp <- as.numeric(train_temp_ts)

feat_data <- create_features(train_power, train_temp, lags)

cat("Feature matrix shape:", nrow(feat_data$X), "x", ncol(feat_data$X), "\n")
cat("Features:", colnames(feat_data$X), "\n")
```

# Hyperparameter Tuning

```{r hyperparameter-tuning}
cat("Running hyperparameter grid search...\n\n")

# Grid of parameters
params_grid <- expand.grid(
  nrounds = c(50, 100, 200),
  max_depth = c(3, 6, 9),
  eta = c(0.01, 0.1, 0.3)
)

actual <- as.numeric(validation_ts)
test_temp <- as.numeric(validation_temp_ts)
n <- length(train_power)

# Store results
tuning_results <- data.frame()
best_rmse <- Inf
best_params <- NULL

for (i in 1:nrow(params_grid)) {
  params <- params_grid[i, ]
  
  # Train XGBoost
  xgb_model <- xgboost(
    data = feat_data$X,
    label = feat_data$y,
    nrounds = params$nrounds,
    max_depth = params$max_depth,
    eta = params$eta,
    objective = "reg:squarederror",
    verbose = 0
  )
  
  # Recursive forecasting
  pred <- numeric(horizon)
  newpower <- train_power
  
  for (j in 1:horizon) {
    # Build features for prediction
    current_n <- length(newpower)
    features <- sapply(lags, function(lag) newpower[current_n - lag + 1])
    features <- c(features, test_temp[j])
    
    pred[j] <- predict(xgb_model, matrix(features, nrow = 1))
    newpower <- c(newpower, pred[j])
  }
  
  # Calculate RMSE
  rmse_val <- sqrt(mean((pred - actual)^2))
  
  tuning_results <- rbind(tuning_results, data.frame(
    nrounds = params$nrounds,
    max_depth = params$max_depth,
    eta = params$eta,
    RMSE = rmse_val
  ))
  
  if (rmse_val < best_rmse) {
    best_rmse <- rmse_val
    best_params <- params
  }
  
  cat(sprintf("nrounds=%d, depth=%d, eta=%.2f -> RMSE=%.2f\n",
              params$nrounds, params$max_depth, params$eta, rmse_val))
}

cat("\n=== BEST PARAMETERS ===\n")
print(best_params)
cat("Best RMSE:", round(best_rmse, 2), "\n")
```

# Train Final Model

```{r final-model}
# Train with best parameters
final_model <- xgboost(
  data = feat_data$X,
  label = feat_data$y,
  nrounds = best_params$nrounds,
  max_depth = best_params$max_depth,
  eta = best_params$eta,
  objective = "reg:squarederror",
  verbose = 1
)
```

# Feature Importance

```{r feature-importance, fig.width=10, fig.height=6}
importance <- xgb.importance(
  feature_names = colnames(feat_data$X),
  model = final_model
)

xgb.plot.importance(importance, main = "XGBoost Feature Importance")

print(importance)
```

# Final Forecast

```{r final-forecast}
# Recursive forecasting with best model
final_pred <- numeric(horizon)
newpower <- train_power

for (j in 1:horizon) {
  current_n <- length(newpower)
  features <- sapply(lags, function(lag) newpower[current_n - lag + 1])
  features <- c(features, test_temp[j])
  
  final_pred[j] <- predict(final_model, matrix(features, nrow = 1))
  newpower <- c(newpower, final_pred[j])
}

# Metrics
test_rmse <- rmse(actual, final_pred)
test_mae <- mae(actual, final_pred)
test_mape <- mape(actual, final_pred) * 100

cat("\n=== VALIDATION SET METRICS ===\n")
cat("RMSE:", round(test_rmse, 2), "\n")
cat("MAE:", round(test_mae, 2), "\n")
cat("MAPE:", round(test_mape, 2), "%\n")
```

# Comparison: With vs Without Temperature

```{r comparison-temp}
cat("\n=== COMPARISON: With vs Without Temperature ===\n\n")

# Train model WITHOUT temperature
feat_no_temp <- create_features(train_power, train_temp, lags)
X_no_temp <- feat_no_temp$X[, -ncol(feat_no_temp$X)]  # Remove temperature column

model_no_temp <- xgboost(
  data = X_no_temp,
  label = feat_no_temp$y,
  nrounds = best_params$nrounds,
  max_depth = best_params$max_depth,
  eta = best_params$eta,
  objective = "reg:squarederror",
  verbose = 0
)

# Forecast without temperature
pred_no_temp <- numeric(horizon)
newpower <- train_power

for (j in 1:horizon) {
  current_n <- length(newpower)
  features <- sapply(lags, function(lag) newpower[current_n - lag + 1])
  
  pred_no_temp[j] <- predict(model_no_temp, matrix(features, nrow = 1))
  newpower <- c(newpower, pred_no_temp[j])
}

cat("RMSE WITH temperature:", round(test_rmse, 2), "\n")
cat("RMSE WITHOUT temperature:", round(rmse(actual, pred_no_temp), 2), "\n")
```

# Visualization

```{r visualization, fig.width=12, fig.height=6}
plot_data <- data.frame(
  Time = 1:horizon,
  Actual = actual,
  `XGBoost (with temp)` = final_pred,
  `XGBoost (no temp)` = pred_no_temp,
  check.names = FALSE
)

plot_long <- plot_data %>%
  pivot_longer(cols = -Time, names_to = "Model", values_to = "Power")

ggplot(plot_long, aes(x = Time, y = Power, color = Model, linetype = Model)) +
  geom_line(linewidth = 0.8) +
  scale_color_manual(values = c("Actual" = "black", 
                                "XGBoost (with temp)" = "darkgreen",
                                "XGBoost (no temp)" = "lightgreen")) +
  scale_linetype_manual(values = c("Actual" = "solid", 
                                   "XGBoost (with temp)" = "dashed",
                                   "XGBoost (no temp)" = "dotted")) +
  labs(title = "XGBoost Forecast Comparison",
       x = "Time (15-min intervals)",
       y = "Power (kW)") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

# Daily Pattern

```{r daily-pattern, fig.width=12, fig.height=6}
time_of_day <- seq(0, 23.75, by = 0.25)

plot_daily <- data.frame(
  Hour = time_of_day,
  Actual = actual,
  Forecast = final_pred
)

ggplot(plot_daily, aes(x = Hour)) +
  geom_line(aes(y = Actual, color = "Actual"), linewidth = 1) +
  geom_line(aes(y = Forecast, color = "XGBoost"), linewidth = 1, linetype = "dashed") +
  scale_x_continuous(breaks = seq(0, 24, by = 4)) +
  labs(title = "XGBoost Forecast - Daily Pattern",
       x = "Hour of Day",
       y = "Power (kW)",
       color = "") +
  theme_minimal()
```

# Save Results

```{r save-results}
xgb_results <- list(
  model = final_model,
  tuning_results = tuning_results,
  best_params = list(
    nrounds = best_params$nrounds,
    max_depth = best_params$max_depth,
    eta = best_params$eta,
    lags = lags
  ),
  feature_importance = importance,
  metrics = data.frame(
    Model = "XGBoost (selected lags + temp)",
    RMSE = test_rmse,
    MAE = test_mae,
    MAPE = test_mape
  ),
  best_model_name = "XGBoost (selected lags + temp)",
  best_rmse = test_rmse,
  best_forecast = final_pred
)

save(xgb_results, file = "../data/xgb_results.RData")
cat("Results saved to ../data/xgb_results.RData\n")
```

# Session Info

```{r session-info}
sessionInfo()
```
